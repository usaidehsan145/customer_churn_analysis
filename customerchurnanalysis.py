# -*- coding: utf-8 -*-
"""CustomerChurnAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cC3P44nKpn_AH4QNvCNcItpGM4FdK9dY
"""

import numpy as np
import pandas as pd
from google.colab import files
import os
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

data = files.upload()

df = pd.read_excel("E Commerce Dataset.xlsx")

df.head()

# Get unique values for each column
for column in df.columns:
    unique_values = df[column].unique()
    print(f"Unique values in {column}: {unique_values}")

# Categorical features
categorical_features = ['PreferredLoginDevice', 'CityTier', 'PreferredPaymentMode', 'Gender',
                         'PreferedOrderCat', 'MaritalStatus', 'Complain']

# Numerical features
numerical_features = ['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'NumberOfDeviceRegistered',
                      'SatisfactionScore', 'NumberOfAddress', 'OrderAmountHikeFromlastYear',
                      'CouponUsed', 'OrderCount', 'DaySinceLastOrder', 'CashbackAmount']

# Set the style for better visualizations
sns.set(style="whitegrid")

# Plot individual graphs for categorical features
for feature in categorical_features:
    plt.figure(figsize=(8, 5))
    sns.countplot(x=feature, hue='Churn', data=df, palette='Set2')
    plt.title(f'{feature} vs. Churn')
    plt.show()

# Plot individual graphs for numerical features
for feature in numerical_features:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x='Churn', y=feature, data=df, palette='Set2')
    plt.title(f'{feature} vs. Churn')
    plt.show()

columns_to_remove_outliers = ['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'NumberOfDeviceRegistered', 'SatisfactionScore', 'NumberOfAddress', 'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount', 'DaySinceLastOrder', 'CashbackAmount']

# Draw boxplots before removing outliers
plt.figure(figsize=(16, 8))
df[columns_to_remove_outliers].boxplot(sym='r+', vert=False)
plt.title('Boxplots Before Removing Outliers')
plt.show()

# Remove outliers using IQR method
for column in columns_to_remove_outliers:
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Draw boxplots after removing outliers
plt.figure(figsize=(16, 8))
df[columns_to_remove_outliers].boxplot(sym='r+', vert=False)
plt.title('Boxplots After Removing Outliers')
plt.show()

# Get unique values for each column
for column in df.columns:
    unique_values = df[column].unique()
    print(f"Unique values in {column}: {unique_values}")

gender_mapping = {'Female': 0, 'Male': 1}
df['Gender'] = df['Gender'].map(gender_mapping)

device_mapping = {'Computer': 0, 'Mobile Phone': 1, 'Phone': 1}
df['PreferredLoginDevice'] = df['PreferredLoginDevice'].map(device_mapping)

df['PreferredPaymentMode'] = df['PreferredPaymentMode'].replace({'CC': 'Credit Card', 'COD': 'Cash on Delivery'})

# Display the frequency of occurrence of each payment type
payment_frequency = df['PreferredPaymentMode'].value_counts()

# Display the result
print("Frequency of occurrence of each payment type:")
print(payment_frequency)

df = pd.get_dummies(df, columns = ['PreferredPaymentMode'],prefix = ' Mode')

# Display the frequency of occurrence of each payment type
order_cat_mapping = {
    'Mobile Phone': 'Mobile'
}

df['PreferedOrderCat'] = df['PreferedOrderCat'].replace(order_cat_mapping)

ordered_frequency = df['PreferedOrderCat'].value_counts()

# Display the result
print("Frequency of occurrence of each ordered type:")
print(ordered_frequency)

# merge others with grocery to avoid maximum biasness in dataset
df['PreferedOrderCat'] = df['PreferedOrderCat'].replace({'Grocery': 'Others'})

# one hot encoding
df = pd.get_dummies(df, columns = ['PreferedOrderCat'],prefix = ' Cat')

# label encoding the maritalStatus

maritalStatus_mapping = {'Single': 0, 'Divorced': 1, 'Married': 2}
df['MaritalStatus'] = df['MaritalStatus'].map(maritalStatus_mapping)

df.head()

correlation_matrix_all = df.corr()
print("Correlation Matrix with All Features:")
print(correlation_matrix_all['Churn'].sort_values(ascending=False))

correlation_matrix = df.corr()

# Set up the matplotlib figure
plt.figure(figsize=(15, 12))

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(correlation_matrix, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": 0.8})

plt.title('Correlation Matrix')
plt.show()

df = df.dropna()

# Check for null values in the cleaned DataFrame
null_values = df.isnull().sum()

# Display columns with null values (if any)
columns_with_null = null_values[null_values > 0].index

if len(columns_with_null) == 0:
    print("No remaining null values in the cleaned dataset.")
else:
    print("Columns with remaining null values:")
    print(columns_with_null)
    print("\nNumber of null values in each column:")
    print(null_values[columns_with_null])

# Drop the 'CustomerID' column
df = df.drop('CustomerID', axis=1)

df.head()

print(df.columns)

print(df['Churn'].value_counts())

# Separate features and target variable
X = df.drop('Churn', axis=1)  # Features
y = df['Churn']  # Target variable

print(X.columns)

print(y.value_counts())

# transform the dataset
oversample = SMOTE()
X, y = oversample.fit_resample(X, y)

print(y.value_counts())

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(y_train.value_counts())

# Standardize the features
scaler = StandardScaler()
X_train_standardized = scaler.fit_transform(X_train)
X_test_standardized = scaler.transform(X_test)

# Initialize Logistic Regression with L1 regularization
logreg_l1 = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)

# Fit the model on the training data
logreg_l1.fit(X_train_standardized, y_train)

# Predictions on the test set
y_pred_l1 = logreg_l1.predict(X_test_standardized)

# Evaluate the model
accuracy_l1 = logreg_l1.score(X_test_standardized, y_test)
print(f"Accuracy with L1 regularization: {accuracy_l1:.4f}")

model_data = {'model': logreg_l1, 'scaler': scaler, 'other_info': 'additional data'}
with open('model.pkl', 'wb') as file:
    pickle.dump(model_data, file)

# Predictions on the test set
y_pred = logreg_l1.predict(X_test_standardized)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(classification_rep)

df_results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print("Actual and Predicted Values Matrix:")
print(df_results)
